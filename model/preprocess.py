import pandas as pd
import numpy as np
import os
import sys
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import pickle
import matplotlib.pyplot as plt

# ThÃªm thÆ° má»¥c config vÃ o path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from config.gesture_classes import GESTURE_CLASSES, get_gesture_name


class DataPreprocessor:
    """Lá»›p xá»­ lÃ½ tiá»n xá»­ lÃ½ dá»¯ liá»‡u cho Hand Gesture Recognition"""
    
    def __init__(self, csv_file_path=None):
        # Tá»± Ä‘á»™ng tÃ¬m Ä‘Æ°á»ng dáº«n Ä‘Ãºng
        if csv_file_path is None:
            # Láº¥y thÆ° má»¥c hiá»‡n táº¡i cá»§a file preprocess.py
            current_dir = os.path.dirname(os.path.abspath(__file__))
            # Äi lÃªn 1 cáº¥p Ä‘á»ƒ vá» thÆ° má»¥c HandGestureRecognition
            project_root = os.path.dirname(current_dir)
            # Táº¡o Ä‘Æ°á»ng dáº«n Ä‘áº¿n dataset
            self.csv_file_path = os.path.join(project_root, 'dataset', 'hand_gestures_dataset.csv')
        else:
            self.csv_file_path = csv_file_path
            
        self.scaler = None
        self.label_encoder = None
        self.df = None
        self.feature_columns = []
    
    def load_data(self):
        """Äá»c dá»¯ liá»‡u tá»« CSV file"""
        print(f"ğŸ“ Loading data from {self.csv_file_path}...")
        
        # Kiá»ƒm tra xem file cÃ³ tá»“n táº¡i khÃ´ng
        if not os.path.exists(self.csv_file_path):
            print(f"âŒ File not found: {self.csv_file_path}")
            
            # Thá»­ tÃ¬m file á»Ÿ cÃ¡c vá»‹ trÃ­ khÃ¡c
            possible_paths = [
                'dataset/hand_gestures_dataset.csv',  # Tá»« thÆ° má»¥c model
                '../dataset/hand_gestures_dataset.csv',  # Tá»« thÆ° má»¥c model
                '../../dataset/hand_gestures_dataset.csv',  # Náº¿u cÃ³ nested folders
                os.path.join(os.getcwd(), 'dataset', 'hand_gestures_dataset.csv'),  # Tá»« current working directory
            ]
            
            print("ğŸ” Searching for dataset file in alternative locations...")
            for path in possible_paths:
                abs_path = os.path.abspath(path)
                print(f"   Checking: {abs_path}")
                if os.path.exists(abs_path):
                    print(f"âœ… Found dataset at: {abs_path}")
                    self.csv_file_path = abs_path
                    break
            else:
                # Náº¿u khÃ´ng tÃ¬m tháº¥y á»Ÿ Ä‘Ã¢u cáº£
                print("\nâŒ Dataset file not found in any expected location!")
                print("ğŸ“‹ Please check:")
                print("   1. File exists: dataset/hand_gestures_dataset.csv")
                print("   2. You have collected some data using the tracking app")
                print("   3. Current working directory:", os.getcwd())
                raise FileNotFoundError(f"Dataset file not found: {self.csv_file_path}")
        
        # Äá»c file CSV
        self.df = pd.read_csv(self.csv_file_path)
        print(f"âœ… Loaded {len(self.df)} samples from {self.csv_file_path}")
        print(f"ğŸ“Š Columns: {len(self.df.columns)} total")
        
        return self.df
    
    def explore_data(self):
        """KhÃ¡m phÃ¡ dá»¯ liá»‡u cÆ¡ báº£n"""
        if self.df is None:
            self.load_data()
            
        print("\n" + "="*60)
        print("ğŸ“Š DATA EXPLORATION")
        print("="*60)
        
        # ThÃ´ng tin cÆ¡ báº£n
        print(f"ğŸ“ˆ Dataset shape: {self.df.shape}")
        print(f"ğŸ“‹ Columns: {len(self.df.columns)}")
        
        # Thá»‘ng kÃª gesture classes
        print("\nğŸ¤š Gesture Distribution:")
        gesture_counts = self.df['gesture_name'].value_counts()
        for gesture, count in gesture_counts.items():
            print(f"  â€¢ {gesture}: {count} samples")
        
        # Thá»‘ng kÃª sá»‘ tay
        print("\nğŸ‘‹ Number of Hands:")
        hand_counts = self.df['num_hands'].value_counts()
        for num_hands, count in hand_counts.items():
            print(f"  â€¢ {num_hands} hand(s): {count} samples")
        
        # Kiá»ƒm tra missing values
        print("\nâ“ Missing Values:")
        missing_data = self.df.isnull().sum()
        missing_cols = missing_data[missing_data > 0]
        if len(missing_cols) > 0:
            print(f"  Found missing values in {len(missing_cols)} columns")
            for col, count in missing_cols.items():
                print(f"    â€¢ {col}: {count} missing")
        else:
            print("  âœ… No missing values found")
    
    def check_data_sufficiency(self):
        """Kiá»ƒm tra xem dá»¯ liá»‡u cÃ³ Ä‘á»§ Ä‘á»ƒ training khÃ´ng"""
        if self.df is None:
            self.load_data()
        
        print("\n" + "="*60)
        print("ğŸ” DATA SUFFICIENCY CHECK")
        print("="*60)
        
        total_samples = len(self.df)
        gesture_counts = self.df['gesture_name'].value_counts()
        min_samples_per_class = gesture_counts.min()
        
        print(f"ğŸ“Š Total samples: {total_samples}")
        print(f"ğŸ“‰ Minimum samples per class: {min_samples_per_class}")
        
        # Äá» xuáº¥t
        recommended_min = 50  # Tá»‘i thiá»ƒu 50 samples má»—i class
        
        if total_samples < 100:
            print("âš ï¸  WARNING: Dataset is very small!")
            print(f"   Recommended: At least 100 total samples")
            print(f"   Current: {total_samples} samples")
        
        if min_samples_per_class < recommended_min:
            print("âš ï¸  WARNING: Some classes have too few samples!")
            print(f"   Recommended: At least {recommended_min} samples per class")
            for gesture, count in gesture_counts.items():
                if count < recommended_min:
                    needed = recommended_min - count
                    print(f"   â€¢ {gesture}: {count} samples (need {needed} more)")
        
        if total_samples >= 100 and min_samples_per_class >= recommended_min:
            print("âœ… Dataset looks good for training!")
        else:
            print("\nğŸ’¡ SUGGESTIONS:")
            print("   1. Collect more data using your tracking app")
            print("   2. Focus on classes with fewer samples")
            print("   3. Consider data augmentation techniques")
            print("   4. Start with a simple model for proof of concept")
        
        return total_samples >= 100 and min_samples_per_class >= recommended_min
    
    def prepare_features(self, use_both_hands=False, normalize_method='minmax'):
        """
        Chuáº©n bá»‹ features tá»« landmark data
        
        Args:
            use_both_hands (bool): Sá»­ dá»¥ng cáº£ 2 tay hay chá»‰ hand_0
            normalize_method (str): 'minmax', 'standard', hoáº·c 'none'
        """
        if self.df is None:
            self.load_data()
        
        print(f"\nğŸ”§ Preparing features (use_both_hands={use_both_hands}, normalize={normalize_method})...")
        
        # Láº¥y cÃ¡c cá»™t landmark
        if use_both_hands:
            # Sá»­ dá»¥ng cáº£ 2 tay
            hand_0_cols = [col for col in self.df.columns if 'hand_0_landmark' in col]
            hand_1_cols = [col for col in self.df.columns if 'hand_1_landmark' in col]
            self.feature_columns = hand_0_cols + hand_1_cols
        else:
            # Chá»‰ sá»­ dá»¥ng hand_0
            self.feature_columns = [col for col in self.df.columns if 'hand_0_landmark' in col]
        
        print(f"ğŸ“Š Selected {len(self.feature_columns)} feature columns")
        
        # Láº¥y features vÃ  xá»­ lÃ½ missing values
        X = self.df[self.feature_columns].fillna(0).values
        
        # Chuáº©n hÃ³a dá»¯ liá»‡u
        if normalize_method == 'minmax':
            self.scaler = MinMaxScaler()
            X_scaled = self.scaler.fit_transform(X)
        elif normalize_method == 'standard':
            self.scaler = StandardScaler()
            X_scaled = self.scaler.fit_transform(X)
        else:
            X_scaled = X
            self.scaler = None
        
        # Láº¥y labels
        y = self.df['gesture_name'].values
        
        # Encode labels
        self.label_encoder = LabelEncoder()
        y_encoded = self.label_encoder.fit_transform(y)
        
        print(f"âœ… Features shape: {X_scaled.shape}")
        print(f"âœ… Labels shape: {y_encoded.shape}")
        print(f"ğŸ“‹ Classes: {list(self.label_encoder.classes_)}")
        
        return X_scaled, y_encoded
    
    def split_data(self, X, y, test_size=0.2, val_size=0.0, random_state=42):
        """
        Chia dá»¯ liá»‡u thÃ nh train/validation/test
        
        Args:
            X: Features
            y: Labels
            test_size: Tá»· lá»‡ test set
            val_size: Tá»· lá»‡ validation set (set to 0 for small datasets)
            random_state: Random seed
        """
        print(f"\nğŸ“‚ Splitting data...")
        
        # Kiá»ƒm tra sá»‘ lÆ°á»£ng samples cho má»—i class
        unique, counts = np.unique(y, return_counts=True)
        min_samples = min(counts)
        
        print(f"ğŸ“Š Class distribution: {dict(zip(unique, counts))}")
        print(f"ğŸ“‰ Minimum samples per class: {min_samples}")
        
        # Náº¿u dataset quÃ¡ nhá» hoáº·c cÃ³ class vá»›i Ã­t samples, khÃ´ng dÃ¹ng stratify
        use_stratify = min_samples >= 2 and len(X) >= 10
        
        if not use_stratify:
            print("âš ï¸ Small dataset detected - using simple random split (no stratification)")
            stratify_param = None
        else:
            stratify_param = y
        
        # Náº¿u val_size = 0 hoáº·c dataset quÃ¡ nhá», chá»‰ chia train/test
        if val_size == 0 or len(X) < 20:
            print(f"ğŸ“‚ Splitting into train/test only (train: {1-test_size:.1f}, test: {test_size:.1f})")
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state, stratify=stratify_param
            )
            X_val, y_val = None, None
        else:
            # Chia train vÃ  temp (chá»©a validation + test)
            print(f"ğŸ“‚ Splitting into train/val/test ({1-test_size-val_size:.1f}/{val_size:.1f}/{test_size:.1f})")
            X_train, X_temp, y_train, y_temp = train_test_split(
                X, y, test_size=test_size+val_size, random_state=random_state, stratify=stratify_param
            )
            
            # Chia temp thÃ nh validation vÃ  test
            val_ratio = val_size / (test_size + val_size)
            temp_stratify = y_temp if use_stratify else None
            X_val, X_test, y_val, y_test = train_test_split(
                X_temp, y_temp, test_size=1-val_ratio, random_state=random_state, stratify=temp_stratify
            )
        
        print(f"âœ… Train set: {X_train.shape[0]} samples")
        if X_val is not None:
            print(f"âœ… Validation set: {X_val.shape[0]} samples")
        print(f"âœ… Test set: {X_test.shape[0]} samples")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def save_preprocessors(self, save_dir='../model/saved_models'):
        """LÆ°u scaler vÃ  label encoder"""
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        
        if self.scaler is not None:
            scaler_path = os.path.join(save_dir, 'scaler.pkl')
            with open(scaler_path, 'wb') as f:
                pickle.dump(self.scaler, f)
            print(f"ğŸ’¾ Saved scaler to {scaler_path}")
        
        if self.label_encoder is not None:
            encoder_path = os.path.join(save_dir, 'label_encoder.pkl')
            with open(encoder_path, 'wb') as f:
                pickle.dump(self.label_encoder, f)
            print(f"ğŸ’¾ Saved label encoder to {encoder_path}")
    
    def load_preprocessors(self, save_dir='../model/saved_models'):
        """Load scaler vÃ  label encoder"""
        scaler_path = os.path.join(save_dir, 'scaler.pkl')
        encoder_path = os.path.join(save_dir, 'label_encoder.pkl')
        
        if os.path.exists(scaler_path):
            with open(scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
            print(f"ğŸ“¥ Loaded scaler from {scaler_path}")
        
        if os.path.exists(encoder_path):
            with open(encoder_path, 'rb') as f:
                self.label_encoder = pickle.load(f)
            print(f"ğŸ“¥ Loaded label encoder from {encoder_path}")


def main():
    """HÃ m main Ä‘á»ƒ test preprocessing"""
    # Khá»Ÿi táº¡o preprocessor
    preprocessor = DataPreprocessor()
    
    # KhÃ¡m phÃ¡ dá»¯ liá»‡u
    preprocessor.explore_data()
    
    # Kiá»ƒm tra tÃ­nh Ä‘á»§ cá»§a dá»¯ liá»‡u
    is_sufficient = preprocessor.check_data_sufficiency()
    
    # Chuáº©n bá»‹ features (chá»‰ dÃ¹ng 1 tay)
    X, y = preprocessor.prepare_features(use_both_hands=False, normalize_method='minmax')
    
    # Chia dá»¯ liá»‡u (khÃ´ng dÃ¹ng validation set cho dataset nhá»)
    X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data(X, y, test_size=0.2, val_size=0.0)
    
    # LÆ°u preprocessors
    preprocessor.save_preprocessors()
    
    if not is_sufficient:
        print("\nâš ï¸  Dataset may be too small for robust training!")
        print("   Consider collecting more data before training a model.")
    
    print("\nğŸ‰ Preprocessing completed successfully!")
    return X_train, X_val, X_test, y_train, y_val, y_test, preprocessor


if __name__ == "__main__":
    X_train, X_val, X_test, y_train, y_val, y_test, preprocessor = main()